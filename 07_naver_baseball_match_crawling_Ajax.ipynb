{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AJax 방식 가져오기\n",
    "\n",
    "#### 페이지 소스에서 안 나올 때, 일부 화면 변경 형식의 데이터일 때\n",
    "#### 개발자 도구 - 네트워크탭  - XHR -  request URL을 확인한다.\n",
    "#### 확인이 가능하면 Ajax, 불가하면 Selenium\n",
    "#### URL : https://sports.news.naver.com/kbaseball/schedule/index.nhn\n",
    "#### Ajax : https://sports.news.naver.com/schedule/scoreBoard.nhn?date=20190317&category=kbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure\n",
    "\n",
    "# 일별 데이터 누적 저장\n",
    "# 1. csv file : 'KBO.csv' (8 variables)\n",
    "# 'status', 'date', 'home_team', 'away_team',\n",
    "# 'home_score','away_score','home_pitcher','away_pitcher'\n",
    "\n",
    "# 중복 추출 방지를 위해 페이지 주소 저장\n",
    "# 2. txt file : 'KBO_log.txt'\n",
    "# urls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4, requests, time, os, re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서버에 페이지 요청하고 soup 반환하는 함수\n",
    "def getSoup(url):\n",
    "    response = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "    return soup\n",
    "\n",
    "# 현재 주의 날짜 리스트를 반환하는 함수\n",
    "def getWeek(soup):\n",
    "    week_date = soup.select('ul.tab > li > a')[1:] # prev(지난 주) 제외\n",
    "    week_date = week_date[:-1] # next(다음 주) 제외\n",
    "    date = []\n",
    "    for day in week_date:\n",
    "        href = day.attrs['onclick']\n",
    "        start = href.find('(')\n",
    "        end = href.find(')')\n",
    "        date.append(href[start+2:end-1])\n",
    "    return date\n",
    "\n",
    "# 다음 주의 첫 날짜를 반환하는 함수\n",
    "def getNext(soup):\n",
    "    next_date = soup.select('li.btn_next2 > a')[0].attrs['onclick']\n",
    "    start = next_date.find('(')\n",
    "    end = next_date.find(')')\n",
    "    date = next_date[start+2:end-1]\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 해당 날짜(date)의 경기를 크롤링하는 함수\n",
    "def getData(date):\n",
    "    url = f'https://sports.news.naver.com/schedule/scoreBoard.nhn?date={date}&category=kbo'\n",
    "    \n",
    "    # 파일에 요청했던 날짜의 url 기록하기\n",
    "    log_filename = 'KBO_log.txt'\n",
    "    # 파일이 없으면 생성\n",
    "    exist_log = os.path.isfile(log_filename)\n",
    "    if exist_log == False:  \n",
    "        with open(log_filename, 'w') as fp:\n",
    "            fp.write('')\n",
    "    # 요청페이지 중복검사 \n",
    "    with open(log_filename, 'r') as rfp:\n",
    "        file = rfp.readlines()\n",
    "        string = ' '.join(file)\n",
    "        chk = string.find(url) # url 중복 검사\n",
    "        if chk == -1:    # 중복되지 않은 페이지일 때 url 기록\n",
    "            with open(log_filename, 'a') as wfp:\n",
    "                str_url = f'{url}\\n'\n",
    "                wfp.writelines(str_url)\n",
    "        else :   # 중복된 요청 페이지일 때 함수 종료\n",
    "            print(f'{date} : exist')\n",
    "            return\n",
    "    \n",
    "    # 크롤링 시작\n",
    "    print(f'{date} : process')\n",
    "    soup = getSoup(url)    \n",
    "    ul_tag = soup.select('ul.sch_vs')\n",
    "    data_all = []\n",
    "    if len(ul_tag) > 0 :\n",
    "        li_list = soup.select('ul.sch_vs > li')\n",
    "        for li_tag in li_list:\n",
    "            data_list = []  # 데이터 한 경기(행) 데이터\n",
    "            class_value = li_tag.attrs['class']\n",
    "            if class_value[0] == 'end' : # 0번째 index로 경기종료,이전 구분\n",
    "                data_list.append('경기종료')\n",
    "                data_list.append(date)\n",
    "                # 팀 데이터 (home,away)\n",
    "                team_tag = li_tag.select('p.vs_team > strong')\n",
    "                data_list.append(team_tag[1].text.strip())\n",
    "                data_list.append(team_tag[0].text.strip())\n",
    "                # 점수 데이터 (home,away)\n",
    "                team_score = li_tag.select('strong.vs_num')\n",
    "                data_list.append(team_score[1].text.strip())\n",
    "                data_list.append(team_score[0].text.strip())\n",
    "                # 투수 데이터 (home, away)\n",
    "                team_pitcher = li_tag.select('span.game_info > a')\n",
    "                if len(team_pitcher) > 0 :\n",
    "                    data_list.append(team_pitcher[1].text.strip())\n",
    "                    data_list.append(team_pitcher[0].text.strip())\n",
    "                else:  \n",
    "                    data_list.append('모름')\n",
    "                    data_list.append('모름')\n",
    "                    \n",
    "            elif class_value[0] == 'before_game':\n",
    "                # 이전 경기 + 경기 취소 (경기취소는 크롤링 후 날짜로 판단 가능)\n",
    "                data_list.append('경기전')\n",
    "                data_list.append(date)\n",
    "                # 팀 데이터 있음\n",
    "                team_tag = li_tag.select('p.vs_team > strong')\n",
    "                data_list.append(team_tag[1].text.strip())\n",
    "                data_list.append(team_tag[0].text.strip())\n",
    "                # 점수 데이터 없음\n",
    "                data_list.append('-1')\n",
    "                data_list.append('-1')\n",
    "                # 투수 데이터 없음\n",
    "                data_list.append('모름')\n",
    "                data_list.append('모름')\n",
    "            #if-elif end\n",
    "            data_all.append(data_list)  # 한 경기(행)를 리스트에 추가\n",
    "        #for end\n",
    "    head_list = ['status', 'date', 'home_team', 'away_team', \n",
    "                 'home_score','away_score','home_pitcher','away_pitcher']\n",
    "    df = pd.DataFrame(data_all, columns = head_list )\n",
    "    filename = 'KBO.csv'    \n",
    "    exists = os.path.isfile(filename)\n",
    "    # if end\n",
    "    \n",
    "    if exists == False:  # 파일이 없으면 헤더 및 내용 추가\n",
    "        df.to_csv(filename, index=False, header=True, encoding='utf-8-sig')\n",
    "    else: # 파일이 있으면 내용 추가\n",
    "        df.to_csv(filename, index=False, header=False, encoding='utf-8-sig',mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190311 : process\n",
      "20190312 : process\n",
      "20190313 : process\n",
      "20190314 : process\n",
      "20190315 : process\n",
      "20190316 : process\n",
      "20190317 : process\n",
      "20190318 : process\n",
      "20190319 : process\n",
      "20190320 : process\n",
      "20190321 : process\n",
      "20190322 : process\n",
      "20190323 : process\n",
      "20190324 : process\n",
      "20190325 : process\n",
      "20190326 : process\n",
      "20190327 : process\n",
      "20190328 : process\n",
      "20190329 : process\n",
      "20190330 : process\n",
      "20190331 : process\n",
      "20190401 : process\n",
      "20190402 : process\n",
      "20190403 : process\n",
      "20190404 : process\n",
      "20190405 : process\n",
      "20190406 : process\n",
      "20190407 : process\n",
      "20190408 : process\n",
      "20190409 : process\n",
      "20190410 : process\n",
      "20190411 : process\n",
      "20190412 : process\n",
      "20190413 : process\n",
      "20190414 : process\n",
      "20190415 : process\n",
      "20190416 : process\n",
      "20190417 : process\n",
      "20190418 : process\n",
      "20190419 : process\n",
      "20190420 : process\n",
      "20190421 : process\n",
      "20190422 : process\n",
      "20190423 : process\n",
      "20190424 : process\n",
      "20190425 : process\n",
      "20190426 : process\n",
      "20190427 : process\n",
      "20190428 : process\n",
      "20190429 : process\n",
      "20190430 : process\n",
      "20190501 : process\n",
      "20190502 : process\n",
      "20190503 : process\n",
      "20190504 : process\n",
      "20190505 : process\n",
      "20190506 : process\n",
      "20190507 : process\n",
      "20190508 : process\n",
      "20190509 : process\n",
      "20190510 : process\n",
      "20190511 : process\n",
      "20190512 : process\n",
      "20190513 : process\n",
      "20190514 : process\n",
      "20190515 : process\n",
      "20190516 : process\n",
      "20190517 : process\n",
      "20190518 : process\n",
      "20190519 : process\n",
      "20190520 : process\n",
      "20190521 : process\n",
      "20190522 : process\n",
      "20190523 : process\n",
      "20190524 : process\n",
      "20190525 : process\n",
      "20190526 : process\n",
      "20190527 : process\n",
      "20190528 : process\n",
      "20190529 : process\n",
      "20190530 : process\n",
      "20190531 : process\n",
      "20190601 : process\n",
      "20190602 : process\n",
      "20190603 : process\n",
      "20190604 : process\n",
      "20190605 : process\n",
      "20190606 : process\n",
      "20190607 : process\n",
      "20190608 : process\n",
      "20190609 : process\n",
      "20190610 : process\n",
      "20190611 : process\n",
      "20190612 : process\n",
      "20190613 : process\n",
      "20190614 : process\n",
      "20190615 : process\n",
      "20190616 : process\n",
      "20190617 : process\n",
      "20190618 : process\n",
      "20190619 : process\n",
      "20190620 : process\n",
      "20190621 : process\n",
      "20190622 : process\n",
      "20190623 : process\n",
      "20190624 : process\n",
      "20190625 : process\n",
      "20190626 : process\n",
      "20190627 : process\n",
      "20190628 : process\n",
      "20190629 : process\n",
      "20190630 : process\n",
      "20190701 : process\n",
      "20190702 : process\n",
      "20190703 : process\n",
      "20190704 : process\n",
      "20190705 : process\n",
      "20190706 : process\n",
      "20190707 : process\n",
      "20190708 : process\n",
      "20190709 : process\n",
      "20190710 : process\n",
      "20190711 : process\n",
      "20190712 : process\n",
      "20190713 : process\n",
      "20190714 : process\n",
      "20190715 : process\n",
      "20190716 : process\n",
      "20190717 : process\n",
      "20190718 : process\n",
      "20190719 : process\n",
      "20190720 : process\n",
      "20190721 : process\n",
      "20190722 : process\n",
      "20190723 : process\n",
      "20190724 : process\n",
      "20190725 : process\n",
      "20190726 : process\n",
      "20190727 : process\n",
      "20190728 : process\n",
      "20190729 : process\n",
      "20190730 : process\n",
      "20190731 : process\n",
      "20190801 : process\n",
      "20190802 : process\n",
      "20190803 : process\n",
      "20190804 : process\n",
      "20190805 : process\n",
      "20190806 : process\n",
      "20190807 : process\n",
      "20190808 : process\n",
      "20190809 : process\n",
      "20190810 : process\n",
      "20190811 : process\n",
      "20190812 : process\n",
      "20190813 : process\n",
      "20190814 : process\n",
      "20190815 : process\n",
      "20190816 : process\n",
      "20190817 : process\n",
      "20190818 : process\n",
      "20190819 : process\n",
      "20190820 : process\n",
      "20190821 : process\n",
      "20190822 : process\n",
      "20190823 : process\n",
      "20190824 : process\n",
      "20190825 : process\n",
      "20190826 : process\n",
      "20190827 : process\n",
      "20190828 : process\n",
      "20190829 : process\n",
      "20190830 : process\n",
      "20190831 : process\n",
      "20190901 : process\n",
      "20190902 : process\n",
      "20190903 : process\n",
      "20190904 : process\n",
      "20190905 : process\n",
      "20190906 : process\n",
      "20190907 : process\n",
      "20190908 : process\n",
      "20190909 : process\n",
      "20190910 : process\n",
      "20190911 : process\n",
      "20190912 : process\n",
      "20190913 : process\n",
      "20190914 : process\n",
      "20190915 : process\n",
      "process done\n"
     ]
    }
   ],
   "source": [
    "# 'start_date'부터 크롤링 시작\n",
    "start_date = 20190311\n",
    "url = f'https://sports.news.naver.com/schedule/scoreBoard.nhn?date={start_date}&category=kbo'\n",
    "i=0 # for Delay\n",
    "while True:\n",
    "    i += 1\n",
    "    #if i == 5: break\n",
    "    soup = getSoup(url)\n",
    "    week = getWeek(soup) # date list\n",
    "    for day in week:\n",
    "        getData(day)\n",
    "    \n",
    "    next_week = getNext(soup)  # a date or None\n",
    "    if len(next_week) == 0:\n",
    "        print('process done')\n",
    "        break\n",
    "    else :\n",
    "        url = f'https://sports.news.naver.com/schedule/scoreBoard.nhn?date={next_week}&category=kbo'\n",
    "        if (i % 3) == 0 : time.sleep(1) # m주 마다 딜레이"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
